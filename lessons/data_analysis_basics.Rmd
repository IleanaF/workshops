
```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
library(knitr)
options(repos="http://cran.rstudio.com/")
opts_chunk$set(fig.path="figures/",R.options=list(max.print=100),message = FALSE,
               warning = FALSE, error = FALSE)
if(!require("ggplot2")){
  install.packages("ggplot2")
}
if(!require("dplyr")){
  install.packages("dplyr")
}
library("ggplot2")
library("dplyr")
nla_url <- "https://usepa.github.io/region1_r/nla_dat.csv"
nla_wq_subset <- read.csv(nla_url,stringsAsFactors = FALSE)
```

# Basic Data analysis with R

This lesson will cover calculating basic statistics with R, conducting statistical tests, building simple linear models, and if there is time, we can talk a bit about `randomForest` .  We will continue to use the 2007 NLA data for the examples (e.g. `nla_wq_subset`)

## Lesson Outline:

- [Basic Statistics](#basic-statist)
- [Data visualization for data analysis](#data-visualization-for-data-analysis)
- [Some tests: t-test and ANOVA](#some-tests-ttest-and-anova)
- [Correlations and Linear modeling](#correlations-and-linear-modeling)
- [Random Forest](#random-forest)

## Lesson Exercises:
- [Exercise 6.1](#exercise-61)
- [Exercise 6.2](#exercise-62)
- [Exercise 6.3](#exercise-63)

## Basic Statistics

First step in analyzing a dataset like this is going to be to dig through some basic statistics as well as some basic plots.  

We can get a summary of the full data frame:

```{r summary, message=FALSE, warning=FALSE}
#Get a summary of the data frame
summary(nla_wq_subset)
```

Or, we can pick and choose what stats we want.  For instance:

```{r individual_stats, message=FALSE, warning=FALSE}
#Stats for Total Nitrogen
mean(nla_wq_subset$NTL)
median(nla_wq_subset$NTL)
min(nla_wq_subset$NTL)
max(nla_wq_subset$NTL)
sd(nla_wq_subset$NTL)
IQR(nla_wq_subset$NTL)
range(nla_wq_subset$NTL)
```

In these cases we took care of our NA values during our data clean up, but there may be reasons you would not want to do that.  If you retained NA values, you would need to think about how to handle those.  One way is to remove it from the calculation of the statistics using the `na.rm = TRUE` argument.  For instance:

```{r na_rm, message=FALSE, warning=FALSE}
#An example with NA's
x <- c(37,22,NA,41,19)
mean(x) #Returns NA
mean(x, na.rm = TRUE) #Returns mean of 37, 22, 41, and 19
```

It is also useful to be able to return some basic counts for different groups.  For instance, how many lakes in the NLA were natural and how many were man made.

```{r table, message=FALSE, warning=FALSE}
#The table() funciton is usefule for returning counts
table(nla_wq_subset$LAKE_ORIGIN)
```

The `table()` function is also useful for looking at multiple columns at once.  A contrived example of that:

```{r table2, message=FALSE, warning=FALSE}
x <- c(1,1,0,0,1,1,0,0,1,0,1,1)
y <- c(1,1,0,0,1,0,1,0,1,0,0,0)
xy_tab <- table(x,y)
xy_tab
prop.table(xy_tab)
```

Lastly, we can use what we learned in the [data aggregation](data_aggregation.md#using-groups-to-summarize-data) lesson and can combine these with some `dplyr` and get summary stats for groups.  

```{r grouping, message=FALSE, warning=FALSE}
orig_stats_ntl <- nla_wq_subset %>%
  group_by(LAKE_ORIGIN) %>%
  summarize(mean_ntl = mean(NTL),
            median_ntl = median(NTL),
            sd_ntl = sd(NTL))
orig_stats_ntl
```

And, just because it is cool, a markdown table!

The code,
```{r tableit, eval=FALSE}
knitr::kable(orig_stats_ntl)
```

produces markdown,

<pre>
```{r tableit2, results= "asis", echo=FALSE}
knitr::kable(orig_stats_ntl)
```
<pre>

which renders in html to something like this.

```{r tableit3, results="asis", echo=FALSE}
knitr::kable(orig_stats_ntl)
```

## Exercise 6.1
1. Look at some of the basic stats for other columns in our data.  What is the standard deviation for PTL?  What is the median Secchi depth?  Play around with others.
2. Using some `dplyr` magic, let's look at mean Secchi by reference class (RT_NLA). 
3. The `quantile()` function allows greater control over getting different quantiles of your data.  For instance you can use it to get the min, median and max with `quantile(nla_wq_subset$NTL, probs = c(0,0.5,1))`.  Re-write this to return the 33rd and 66th quantiles.

## Data visualization for data analysis

While we have already covered visualization and talked about these, I wanted to include them here as they really are an integral part of exploratory data analysis. In particular distributions and bi-variate relationships are better displayed graphically.  

We can look at histograms and density:

```{r histogram_density, message=FALSE, warning=FALSE}
#A single histogram using base
hist(nla_wq_subset$NTL)
#Log transform it
hist(log1p(nla_wq_subset$NTL)) #log1p adds one to deal with zeros
#Density plot
plot(density(log1p(nla_wq_subset$NTL)))
```

And boxplots:

```{r boxplots, message=FALSE, warning=FALSE}
#Simple boxplots
boxplot(nla_wq_subset$CHLA)
boxplot(log1p(nla_wq_subset$CHLA))

#Boxplots per group
boxplot(log1p(nla_wq_subset$CHLA)~nla_wq_subset$EPA_REG)
```

And scatterplots:

```{r scatterplots, message=FALSE, warning=FALSE}
#A single scatterplot
plot(log1p(nla_wq_subset$PTL),log1p(nla_wq_subset$CHLA))
#A matrix of scatterplot
plot(log1p(nla_wq_subset[,6:9]))
```

Lastly, it might be nice to look at these on a per variable basis or on some grouping variable. First we could look at the density of each measured variable. This requires some manipulation of the data which will allow us to use facets in ggplot to create a density distribution for each of the variables.

```{r fancy_density, message=FALSE, warning=FALSE}
#Getting super fancy with tidyr, plotly (commented out for md), and ggplot2 to visualize all variables
library(tidyr)
library(ggplot2)
library(plotly)
nla_gather <- gather(nla_wq_subset,parameter,value,6:9)
dens_gg <-ggplot(nla_gather,aes(x=log1p(value))) +
  geom_density() +
  facet_wrap("parameter") +
  labs(x="log1p of measured value")
ggplotly(dens_gg)
#dens_gg
```

Next we could look at a scatterplot matrix of the relationship between phosphorus and chlorophyl by each EPA Region.  No need to re-do the shape of the data frame for this one.

```{r fancy_matrix, message=FALSE, warning=FALSE}
ggplot(nla_wq_subset, aes(x=log1p(PTL),y=log1p(NTL))) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap("EPA_REG")
```

### Exercise 6.2
1. Build a scatterplot that looks at the relationship between PTL and NTL.  
2. Build a boxplot that shows a boxplot of secchi by the reference class (RT_NLA)

## Some tests: t-test and ANOVA
There are way more tests than we can show examples for.  For today we will show two very common and straightforward tests.  The t-test and an ANOVA.

### t-test
First we will look at the t-test to test and see if `LAKE_ORIGIN` shows a difference in `SECMEAN`.  In other words can we expect a difference in clarity due to whether a lake is man-made or natural.  This is a two-tailed test. There are two approaches for this 1) using the formula notation if your dataset is in a "long" format or 2) using two separate vectors if your dataset is in a "wide" format.

```{r t-test, message=FALSE, warning=FALSE}
#Long Format - original format for LAKE_ORIGIN and SECMEAN
t.test(nla_wq_subset$SECMEAN ~ nla_wq_subset$LAKE_ORIGIN)

#Wide Format - need to do some work to get there - tidyr is handy!
wide_nla <- spread(nla_wq_subset,LAKE_ORIGIN,SECMEAN)
names(wide_nla)[8:9]<-c("man_made", "natural")
t.test(wide_nla$man_made, wide_nla$natural)
```

Same results, two different ways to approach.  Take a look at the help (e.g. `?t.test`) for more details on other types of t-tests (e.g. paired, one-tailed, etc.)

### ANOVA
ANOVA can get involved quickly and I haven't done them since my last stats class, so I'm not the best to talk about these, but the very basics require fitting a model and wrapping that in the `aov` function.  In the [Getting More Help section](#getting-more-help) I provide a link that would be a good first start for you ANOVA junkies.  For today's lesson though, lets look at the simple case of a one-vay analysis of variance and check if reference class results in differences in our chlorophyll

```{r simple_anova, message=FALSE, warning=FALSE}
# A quick visual of this:
boxplot(log1p(nla_wq_subset$CHLA)~nla_wq_subset$RT_NLA)

# One way analysis of variance
nla_anova <- aov(log1p(CHLA)~RT_NLA, data=nla_wq_subset)
nla_anova #Terms
summary(nla_anova) #The table
anova(nla_anova) #The table with a bit more
```

So now that we know a difference exists, we can look at this more carefully, perhaps with a Tukey's Honest Significance Test.

```{r bonferonni}
tukey_test <- TukeyHSD(nla_anova, ordered = TRUE, conf.level = 0.95)
tukey_test
summary(tukey_test)
```

## Exercise 6.3

1. Run an ANOVA to see if mean chlorophyll differs across EPA Regions (EPA_REG).
2. Run a Tukey's HSD to look at the per region comparisons.

## Correlations and Linear modeling
The last bit of basic stats we will cover is going to be linear relationships.

### Correlations
Let's first take a look at correlations.  These can be done with `cor()`.

```{r cor, message=FALSE, warning=FALSE}
#For a pair
cor(log1p(nla_wq_subset$PTL),log1p(nla_wq_subset$NTL))
#For a correlation matrix
cor(log1p(nla_wq_subset[,6:9]))
#Spearman Rank Correlations
cor(log1p(nla_wq_subset[,6:9]),method = "spearman")
```

You can also test for differences using:

```{r cor_test, message=FALSE,warning=FALSE}
cor.test(log1p(nla_wq_subset$PTL),log1p(nla_wq_subset$NTL))
```

### Linear models
Basic linear models in R can be built with the `lm()` function.  If you aren't buiding stadard least squares regressin models, (e.g. logistic) or aren't doing linear models then you will need to look elsewhere (e.g `glm()`, or `nls()`).  For today our focus is going to be on simple linear models.  Let's look at our ability to model chlorophyll, given the other variables we have.

```{r}
# The simplest case
chla_tp <- lm(log1p(CHLA) ~ log1p(PTL), data=nla_wq_subset) #Creates the model
summary(chla_tp) #Basic Summary
names(chla_tp) #The bits
chla_tp$coefficients #My preference
coef(chla_tp) #Same thing, but from a function
head(resid(chla_tp)) # The resdiuals
```

We can also do multiple linear regression.

```{r multiple, warning=FALSE, message=FALSE}
chla_tp_tn_turb <- lm(log1p(CHLA) ~ log1p(PTL) + log1p(NTL), data = nla_wq_subset)
summary(chla_tp_tn_turb)
```

There's a lot more we can do with linear models including dummy variables (character or factors will work), interactions, etc.  That's a bit more than we want to get into.  Again the link below is a good place to start for more info.

### Exercise 6.4
1. Use `lm()` to look at using secchi depth to predict chlorophyll.

## Random Forest

Random forests were developed by Leo Breiman at UC-Berkely ([Breiman 2001](http://dx.doi.org/10.1023/A:1010933404324)).  At their simplest, a random forest is a collection (or ensemble) of classification (or regression) trees.  Each tree is developed based on a resampling of both the data and variables.  They have been shown to have superior predictive perfomance as well as to be robust to violation of the standard suite of assumptions that often plague other approaches.  In my opinion, if you want to learn one machine learning approach, this is it.  If you want to read more, a few good references are:

- [Cutler et. al. (2007):Random forests for classification in ecology](http://dx.doi.org/10.1890/07-0539.1)
- [Fern´andez-Delgado et al. 2014: Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) (HT:[Simply Statistics](http://simplystatistics.org/2014/12/17/a-non-comprehensive-list-of-awesome-things-other-people-did-in-2014/))

The implementation in R is accessed through the `randomForest` package.  Let's install this, load it up and start to look around.

```{r rf_actual_install, echo=FALSE, include=FALSE, purl=FALSE}
if(!require("randomForest")){
  install.packages("randomForest")
}
library("randomForest")
```

```{r rf_install, eval=FALSE}
install.packages("randomForest")
library("randomForest")
help(package="randomForest")
```

There are quite a few functions included with this package, but the one we are most interested in is the workhorse `randomForest()` function.  For our examples we will look at classification trees.  The bare minimum we need for this to work are just our independent (x) and dependent variables (y).  Let's see if we can predict iris species from the morphology measurements...

```{r rf_example}
rf_x<-select(iris,Petal.Width, Petal.Length, Sepal.Width, Sepal.Length)
rf_y<-iris$Species
iris_rf<-randomForest(x=rf_x,y=rf_y)
iris_rf
```

Can we predict species, well, I'd say so!  

We can also call `randomForest()` using a formula for a data frame like,

```{r rf_form_examp}
iris_rf2<-randomForest(Species~.,data=iris)
iris_rf2
```

The only thing we haven't seen before here is the ".".  That indicates all the other variables in the data frame.  It is just a shortcut and you could explicitly name the variables to use.

Lastly, we can look at a couple of plots that tell us a little bit more about the model with some of the plotting functions that come with randomForest.

```{r rf_plots}
#Error vs num of trees
plot(iris_rf2)
#Variable Importance
varImpPlot(iris_rf2)
```

The default plot shows error for the total model and and each of the classes as a function of the number of trees. Variable importance is plotting the decrease in Gini as each variable is added to the model.  

For the basics, that is it! 

## Exercise 6.5

For this exercise we are going to use a random forest and try to predict the NLA reference class from the water quality measurements.

1. Add a new section to the script
2. Build your model to predict RT_NLA.  You may specify the model however makes the most sense
3. Print to the screen your result, plot the error and variable importance. When finished put a green sticky up and we will come around to check.

## Getting More Help

One nice site that covers basic stats in R is [Quick R: Basic Statistics](http://www.statmethods.net/stats/index.html).  There are others, but that is a good first stop.